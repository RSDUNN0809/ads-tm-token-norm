{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# ADS 509 Assignment 2.1: Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae8e2e1",
   "metadata": {},
   "source": [
    "## General Assignment Instructions\n",
    "\n",
    "These instructions are included in every assignment, to remind you of the coding standards for the class. Feel free to delete this cell after reading it. \n",
    "\n",
    "One sign of mature code is conforming to a style guide. We recommend the [Google Python Style Guide](https://google.github.io/styleguide/pyguide.html). If you use a different style guide, please include a cell with a link. \n",
    "\n",
    "Your code should be relatively easy-to-read, sensibly commented, and clean. Writing code is a messy process, so please be sure to edit your final submission. Remove any cells that are not needed or parts of cells that contain unnecessary code. Remove inessential `import` statements and make sure that all such statements are moved into the designated cell. \n",
    "\n",
    "Make use of non-code cells for written commentary. These cells should be grammatical and clearly written. In some of these cells you will have questions to answer. The questions will be marked by a \"Q:\" and will have a corresponding \"A:\" spot for you. *Make sure to answer every question marked with a `Q:` for full credit.* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2d096b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "#import tweepy\\n\",\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# for the lyrics scrape section\\n\",\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "   \n",
    "# Use this cell for any import statements you add\\n\",\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import * \n",
    "    \n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = '/Users/ryan_s_dunn/opt/anaconda3/lib/python3.8/posixpath.py'\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06522af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = 0\n",
    "    num_unique_tokens = 0\n",
    "    lexical_diversity = 0.0\n",
    "    num_characters = 0\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "d8e4ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 8 unique tokens in the data.\n",
      "There are 25 characters in the data.\n",
      "The lexical diversity is 8.000 in the data.\n",
      "[('country', 3), ('6', 2), ('run', 2), (\"'d\", 2), ('3', 1), ('runs', 1), ('runner', 1), ('I', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[13, 8, 8.0, 25]"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "punctuation = set(punctuation)\n",
    "\n",
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True):\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    #lists and counters for iterating though the list of words \\n\",\n",
    "    total_type_list = []\n",
    "    full_token_list = []\n",
    "    lemmas = []\n",
    "    num_tokens = 0\n",
    "    num_unique_tokens = 0\n",
    "    lexical_diversity = 0.0\n",
    "    num_characters = 0\n",
    "\n",
    "    #create the total tokens variable for all words passed into the function\n",
    "    all_tokens = nltk.tokenize.word_tokenize(tokens)\n",
    "    \n",
    " #   all_tokens2 = []\n",
    " #   for token in all_tokens:\n",
    " #       clean_token = re.sub(\"[^a-zA-Z' ]+\", '', token)\n",
    " #       all_tokens2.append(clean_token)\n",
    "    \n",
    "    #create a total_tokens_clean list to store only words that are not stopwords\n",
    "    all_tokens_clean = []\n",
    "    for token in all_tokens:\n",
    "        if token in stopwords:\n",
    "            pass\n",
    "        else:\n",
    "             all_tokens_clean.append(token)\n",
    "\n",
    "    #append the total_tokens_clean list to remove any words found in the punctuation list\n",
    "    all_tokens_clean = [token for token in all_tokens_clean if token not in punctuation]\n",
    "\n",
    "    final_clean = []\n",
    "    for token in all_tokens_clean:\n",
    "        if token in ('``','\"\"',\"''\",'‚Ä¢','nan','...',\"'\",\"'s\",'‚Äô'):\n",
    "            pass   \n",
    "        else:\n",
    "            final_clean.append(token)\n",
    "\n",
    "        \n",
    "    #collect all types in the data set after cleaning for stopwords and punctuation\n",
    "    for types in final_clean:\n",
    "        total_type_list.append(types)\n",
    "#    print('These are the total types',total_type_list)\n",
    "\n",
    "    #iterate through each token and append/count to the appropriate lists\n",
    "    for token in final_clean:\n",
    "        num_tokens += 1\n",
    "        if token in full_token_list:        \n",
    "            num_unique_tokens +=0\n",
    "        else:\n",
    "            num_unique_tokens += 1\n",
    "            full_token_list.append(token)\n",
    "            for character in token:\n",
    "                num_characters += 1\n",
    "    \n",
    " #   print('This is the full token list', full_token_list) \n",
    "\n",
    "    #iterate through each individual token in the list and develop the lemmas where applicable\n",
    "    for individual_token in full_token_list:\n",
    "        lem = lemmatizer.lemmatize(individual_token)    \n",
    "        if lem in lemmas:\n",
    "            lexical_diversity += 0\n",
    "        else:\n",
    "            lexical_diversity += 1\n",
    "            lemmas.append(individual_token)\n",
    "            \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "\n",
    "    #collect the 5 most common tokens\n",
    "    counter_list = Counter(final_clean)\n",
    "    top_5 = counter_list.most_common(10)\n",
    "    print(top_5)\n",
    "\n",
    "    return([num_tokens, num_unique_tokens,lexical_diversity,num_characters])\n",
    "              \n",
    "descriptive_stats(\"here is country country country 3 6 6 with `` '' \"\" runs run run's runner & that's what's I'd do for you'd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "59dcf058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5 tokens in the data.\n",
      "There are 2 unique tokens in the data.\n",
      "There are 11 characters in the data.\n",
      "The lexical diversity is 2.000 in the data.\n",
      "[('text', 3), ('example', 2)]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-417-a93259e22cda>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"here is some example text with other example text here in this text\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.69\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0.02\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescriptive_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m55\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "text = \"here is some example text with other example text here in this text\"\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22286a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: <!-- Your answer here --> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02c9e24",
   "metadata": {},
   "source": [
    "##### Import Cher Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "98ca204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Cher links are: 316\n"
     ]
    }
   ],
   "source": [
    "#define the filepath for cher lyrics\n",
    "cher_path = '/Users/ryan_s_dunn/Documents/USD MS-ADS/Applied Text Mining ADS 509/Module 2/M1 Results/lyrics/cher'\n",
    "txt_files = glob.glob(os.path.join(cher_path, '*.txt'))\n",
    "\n",
    "cher_data = {}\n",
    "for file in txt_files:\n",
    "    with open(file, 'r') as f:\n",
    "        cher_data[file] = f.read()\n",
    "\n",
    "#collect the file paths for cher\n",
    "cher_links = []\n",
    "for text_file in cher_data:\n",
    "    cher_links.append(text_file)\n",
    "print(\"Total Cher links are:\" ,len(cher_links))\n",
    "\n",
    "#get all lines from cher songs\n",
    "cher_total_lines = []\n",
    "for link in cher_links:\n",
    "    with open(link, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        cher_total_lines.append(lines)\n",
    "\n",
    "cher_total_lines = str(cher_total_lines)\n",
    "#print(\"Total of all words for Cher:\",len(cher_total_lines))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4fc794",
   "metadata": {},
   "source": [
    "##### Import Robyn Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "e464d4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Robyn links are: 104\n"
     ]
    }
   ],
   "source": [
    "#define the filepath for robyn lyrics\n",
    "cher_path = '/Users/ryan_s_dunn/Documents/USD MS-ADS/Applied Text Mining ADS 509/Module 2/M1 Results/lyrics/robyn'\n",
    "txt_files = glob.glob(os.path.join(cher_path, '*.txt'))\n",
    "\n",
    "robyn_data = {}\n",
    "for file in txt_files:\n",
    "    with open(file, 'r') as f:\n",
    "        robyn_data[file] = f.read()\n",
    "\n",
    "#collect the file paths for cher\n",
    "robyn_links = []\n",
    "for text_file in robyn_data:\n",
    "    robyn_links.append(text_file)\n",
    "print(\"Total Robyn links are:\" ,len(robyn_links))\n",
    "\n",
    "#get all lines from robyn songs\n",
    "robyn_total_lines = []\n",
    "for link in robyn_links:\n",
    "    with open(link, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "        robyn_total_lines.append(lines)\n",
    "\n",
    "robyn_total_lines = str(robyn_total_lines)\n",
    "#print(\"Total of all words for Cher:\",len(cher_total_lines))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b59cf66",
   "metadata": {},
   "source": [
    "##### Import Cher Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "4f0fe7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 624: expected 7 fields, saw 12\\nSkipping line 17506: expected 7 fields, saw 12\\nSkipping line 104621: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 188924: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 301600: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 429936: expected 7 fields, saw 12\\nSkipping line 444405: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 677792: expected 7 fields, saw 12\\nSkipping line 773482: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 818258: expected 7 fields, saw 12\\nSkipping line 895225: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 955213: expected 7 fields, saw 10\\nSkipping line 994827: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1246039: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1569117: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2127250: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2335031: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2681065: expected 7 fields, saw 10\\n'\n",
      "b'Skipping line 3147696: expected 7 fields, saw 12\\n'\n"
     ]
    }
   ],
   "source": [
    "cher_twitter = pd.read_csv('/Users/ryan_s_dunn/Documents/USD MS-ADS/Applied Text Mining ADS 509/Module 2/M1 Results/twitter/cher_followers_data.txt'\n",
    "                           , header = 0, sep = '\\t' \n",
    "                           , names = [\"screen_name\",\"name\",\"id\",\"location\",\"followers_count\",\"friends_count\",\"description\"]\n",
    "                           , error_bad_lines = False)\n",
    "#cher_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "8a550500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3916302, 1)"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cher_desc = cher_twitter[['description']]\n",
    "cher_desc = cher_desc.iloc[:10000000]\n",
    "cher_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "415921a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_twitter_list = cher_desc.values.tolist()\n",
    "cher_twitter_list = cher_twitter_list\n",
    "#cher_twitter_list #do not print this "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc23db81",
   "metadata": {},
   "source": [
    "##### Import Robyn Twitter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "2e882d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_twitter = pd.read_csv('/Users/ryan_s_dunn/Documents/USD MS-ADS/Applied Text Mining ADS 509/Module 2/M1 Results/twitter/robynkonichiwa_followers_data.txt'\n",
    "                           , header = 0, sep = '\\t' \n",
    "                           , names = [\"screen_name\",\"name\",\"id\",\"location\",\"followers_count\",\"friends_count\",\"description\"]\n",
    "                           , error_bad_lines = False)\n",
    "#cher_twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "6f7b0671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(351839, 1)"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "robyn_desc = robyn_twitter[['description']]\n",
    "robyn_desc = robyn_desc.iloc[:1000000]\n",
    "robyn_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "8d7d9f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "robyn_twitter_list = robyn_desc.values.tolist()\n",
    "robyn_twitter_list = robyn_twitter_list\n",
    "#robyn_twitter_list #do not print this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "b9b8b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cher data prep for twitter\n",
    "cher_full_word_list = []\n",
    "for ind_list in cher_twitter_list:\n",
    "    for word in ind_list:\n",
    "        cher_full_word_list.append(word)\n",
    "        \n",
    "cher_full_word_list = str(cher_full_word_list)\n",
    "#cher_full_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "722232d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#robyn data prep for twitter\n",
    "robyn_full_word_list = []\n",
    "for ind_list in robyn_twitter_list:\n",
    "    for word in ind_list:\n",
    "        robyn_full_word_list.append(word)\n",
    "        \n",
    "robyn_full_word_list = str(robyn_full_word_list)\n",
    "#robyn_full_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea142a",
   "metadata": {},
   "source": [
    "##### note: data cleaning steps are included within the descriptive statistics function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad65e00",
   "metadata": {},
   "source": [
    "##### Lyrics descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "f0bbedd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher lyrics descriptive statistic: \n",
      "There are 46744 tokens in the data.\n",
      "There are 4929 unique tokens in the data.\n",
      "There are 29106 characters in the data.\n",
      "The lexical diversity is 4702.000 in the data.\n",
      "[('I', 3587), (\"n't\", 1003), ('love', 859), (\"'m\", 514), ('know', 478), (\"'And\", 478), (\"'re\", 431), ('na', 353), (\"'ll\", 351), ('You', 323)]\n",
      "\n",
      "\n",
      "Robyn lytrics descriptive statistic: \n",
      "There are 19181 tokens in the data.\n",
      "There are 2699 unique tokens in the data.\n",
      "There are 15670 characters in the data.\n",
      "The lexical diversity is 2633.000 in the data.\n",
      "[('I', 1271), (\"n't\", 513), ('know', 303), (\"'m\", 298), ('got', 248), ('love', 246), ('like', 209), (\"'re\", 198), ('na', 180), (\"'We\", 167)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[19181, 2699, 2633.0, 15670]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calls to descriptive_stats here\n",
    "print(\"Cher lyrics descriptive statistic: \")\n",
    "descriptive_stats(cher_total_lines)\n",
    "print('\\n')\n",
    "print(\"Robyn lytrics descriptive statistic: \")\n",
    "descriptive_stats(robyn_total_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dc0acd",
   "metadata": {},
   "source": [
    "##### Twitter descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a150f9",
   "metadata": {},
   "source": [
    "## DO NOT RUN THIS CELL AGAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "3e2ac0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cher twitter descriptions descriptive statistics\n"
     ]
    }
   ],
   "source": [
    "print(\"Cher twitter descriptions descriptive statistics\")\n",
    "#descriptive_stats(cher_full_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8eeba2",
   "metadata": {},
   "source": [
    "## DO NOT RUN THIS CELL AGAIN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "c9fec183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robyn twitter descriptions descriptive statistics\n",
      "There are 1632063 tokens in the data.\n",
      "There are 326118 unique tokens in the data.\n",
      "There are 3240791 characters in the data.\n",
      "The lexical diversity is 323105.000 in the data.\n",
      "[('I', 32617), ('music', 9028), ('love', 7862), ('och', 7777), (\"'m\", 7164), ('https', 7039), ('de', 6045), ('life', 5911), ('p√•', 4653), ('like', 4490)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1632063, 326118, 323105.0, 3240791]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Robyn twitter descriptions descriptive statistics\")\n",
    "descriptive_stats(robyn_full_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Leaving stop words in the data would essentially flod the top 5 list with words that essentially do not help to interpret any really meaning in the aggregate. \n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "50f068a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"‚ù§Ô∏è\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis üòÅ\n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a206d68",
   "metadata": {},
   "source": [
    "##### Cher most common emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "f059bd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full word list for cher\n",
    "cher_all_tokens = nltk.tokenize.word_tokenize(cher_full_word_list)\n",
    "\n",
    "#create for loop to find all token classified as emoji in the cher tokens\n",
    "cher_emoji_list = [] \n",
    "for token in cher_all_tokens:\n",
    "    for type in token:\n",
    "        if emoji.is_emoji(token) == True:\n",
    "            cher_emoji_list.append(token)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "6d523b61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ù§Ô∏è', 28472),\n",
       " ('‚ô•', 9918),\n",
       " ('‚ù§', 9447),\n",
       " ('‚ú®', 7506),\n",
       " ('üá∫üá∏', 7098),\n",
       " ('üåà', 4949),\n",
       " ('üá∫üá¶', 3640),\n",
       " ('üíô', 3627),\n",
       " ('üá®üá¶', 3450),\n",
       " ('üíú', 3364)]"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#put the emoji counter here:\n",
    "#count each emoji type and return the most common \n",
    "cher_count_emojis = []\n",
    "cher_counter_emoji = Counter(cher_emoji_list)\n",
    "top_10_emoji_cher = cher_counter_emoji.most_common(10)\n",
    "top_10_emoji_cher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0290a983",
   "metadata": {},
   "source": [
    "##### Robyn most common emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "1e29592d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full word list for robyn\n",
    "all_tokens = nltk.tokenize.word_tokenize(robyn_full_word_list)\n",
    "\n",
    "#create for loop to find all token classified as emoji in the robyn tokens\n",
    "robyn_emoji_list = [] \n",
    "for token in all_tokens:\n",
    "    for type in token:\n",
    "        if emoji.is_emoji(token)  == True:\n",
    "            robyn_emoji_list.append(token)\n",
    "        else:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "91840e6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('‚ù§Ô∏è', 1864),\n",
       " ('‚ô•', 1158),\n",
       " ('‚ú®', 678),\n",
       " ('‚ù§', 618),\n",
       " ('üåà', 517),\n",
       " ('üá∫üá∏', 298),\n",
       " ('‚úåÔ∏è', 298),\n",
       " ('üá¨üáß', 294),\n",
       " ('üá≤üáΩ', 256),\n",
       " ('üá®üá¶', 254)]"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count each emoji type and return the most common    \n",
    "count_emojis = []\n",
    "counter_emoji = Counter(robyn_emoji_list)\n",
    "top_10_emoji = counter_emoji.most_common(10)\n",
    "top_10_emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0246d8a7",
   "metadata": {},
   "source": [
    "##### Cher Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "b88d4ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[#DCnative]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[#TheResistance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>[#Pagan.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>[#BlackLivesMatter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>[#COWBOY]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            description\n",
       "4           [#DCnative]\n",
       "9      [#TheResistance]\n",
       "34            [#Pagan.]\n",
       "67  [#BlackLivesMatter]\n",
       "77            [#COWBOY]"
      ]
     },
     "execution_count": 400,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cast the description field as str in the robyn dataframe\n",
    "cher_desc['description'] = cher_desc['description'].astype(str)\n",
    "\n",
    "#lambda function to find words with #\n",
    "hashtag_words = cher_desc['description'].str.split().apply(lambda hashtags: \n",
    "                                                            [word for word in hashtags if word.startswith('#')])\n",
    "#set the hashtag_words variable as a dataframe\n",
    "hashtag_words = pd.DataFrame(hashtag_words)\n",
    "\n",
    "#remove the empty lists from the dataframe\n",
    "hashtag_words = hashtag_words[hashtag_words['description'].map(lambda d: len(d)) > 0]\n",
    "hashtag_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "12fb1f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "\n",
    "# iterate over the rows of the DataFrame\n",
    "for _, row in hashtag_words.iterrows():\n",
    "    for hashtag in row:\n",
    "        word_list.append(hashtag)\n",
    "\n",
    "#iterate through each sublist in each row and store it a new list\n",
    "word_list = [word for sublist in word_list for word in sublist]\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e4b86d22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#BLM', 7917),\n",
       " ('#Resist', 5003),\n",
       " ('#BlackLivesMatter', 4221),\n",
       " ('#resist', 3129),\n",
       " ('#FBR', 2765),\n",
       " ('#blacklivesmatter', 2464),\n",
       " ('#TheResistance', 2457),\n",
       " ('#1', 2228),\n",
       " ('#', 1976),\n",
       " ('#Resistance', 1514)]"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count each hastag type and return the most common    \n",
    "count_hashtags = []\n",
    "counter_hashtags = Counter(word_list)\n",
    "top_10_hashtags = counter_hashtags.most_common(10)\n",
    "top_10_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec766fdf",
   "metadata": {},
   "source": [
    "##### Robyn Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "b258c22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[#morecomingsoon...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>[#WorldOfWarcraft, #HipHop, #Underground., #Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>[#MohammadTheCat, #VanGoghTheCat, #KendraTheHu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>[#BeingAWomanIs]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>[#Antifa, #ASRoma, #StraightOuttaMonteverde, #...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          description\n",
       "2                                [#morecomingsoon...]\n",
       "22  [#WorldOfWarcraft, #HipHop, #Underground., #Ma...\n",
       "55  [#MohammadTheCat, #VanGoghTheCat, #KendraTheHu...\n",
       "71                                   [#BeingAWomanIs]\n",
       "78  [#Antifa, #ASRoma, #StraightOuttaMonteverde, #..."
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#cast the description field as str in the robyn dataframe\n",
    "robyn_desc['description'] = robyn_desc['description'].astype(str)\n",
    "\n",
    "#lambda function to find words with #\n",
    "hashtag_words = robyn_desc['description'].str.split().apply(lambda hashtags: \n",
    "                                                            [word for word in hashtags if word.startswith('#')])\n",
    "#set the hashtag_words variable as a dataframe\n",
    "hashtag_words = pd.DataFrame(hashtag_words)\n",
    "\n",
    "#remove the empty lists from the dataframe\n",
    "hashtag_words = hashtag_words[hashtag_words['description'].map(lambda d: len(d)) > 0]\n",
    "hashtag_words.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "0a3b04e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "\n",
    "# iterate over the rows of the DataFrame\n",
    "for _, row in hashtag_words.iterrows():\n",
    "    for hashtag in row:\n",
    "        word_list.append(hashtag)\n",
    "\n",
    "#iterate through each sublist in each row and store it a new list\n",
    "word_list = [word for sublist in word_list for word in sublist]\n",
    "#print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "2c9a7ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#BlackLivesMatter', 311),\n",
       " ('#BLM', 274),\n",
       " ('#blacklivesmatter', 200),\n",
       " ('#1', 187),\n",
       " ('#', 151),\n",
       " ('#music', 150),\n",
       " ('#Music', 93),\n",
       " ('#EDM', 79),\n",
       " ('#blm', 51),\n",
       " ('#TeamFollowBack', 51)]"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count each hastag type and return the most common    \n",
    "count_hashtags = []\n",
    "counter_hashtags = Counter(word_list)\n",
    "top_10_hashtags = counter_hashtags.most_common(10)\n",
    "top_10_hashtags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d34c82",
   "metadata": {},
   "source": [
    "##### Cher Song Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "f06f2371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all first lines from cher songs\n",
    "cher_first_lines = []\n",
    "for link in cher_links:\n",
    "    with open(link, 'r') as f:\n",
    "        first_line = f.readline().rstrip()\n",
    "        cher_first_lines.append(first_line)\n",
    "        \n",
    "#cher_first_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "8cdce903",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of songs from the first lines of the .txt files\n",
    "song_list = [song.replace('\"', '') for song in cher_first_lines]\n",
    "song_list = [song.replace(\"'\", '') for song in song_list]\n",
    "#song_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "30d1d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the individual words and clean for punctuation \n",
    "cher_title_tokens = nltk.tokenize.word_tokenize(str(song_list))\n",
    "cher_title_tokens = [token for token in cher_title_tokens if token not in punctuation]\n",
    "#cher_title_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('You', 35),\n",
       " ('I', 32),\n",
       " ('The', 30),\n",
       " ('To', 28),\n",
       " ('Love', 26),\n",
       " ('Me', 25),\n",
       " ('A', 24),\n",
       " (\"'The\", 24),\n",
       " ('Of', 21),\n",
       " ('And', 13)]"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count each hastag type and return the most common    \n",
    "count_song_title_words = []\n",
    "counter_title = Counter(cher_title_tokens)\n",
    "top_10_words = counter_title.most_common(10)\n",
    "top_10_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58e6b24",
   "metadata": {},
   "source": [
    "##### Robyn Song Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "7be9bea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get all first lines from robyn songs\n",
    "robyn_first_lines = []\n",
    "for link in robyn_links:\n",
    "    with open(link, 'r') as f:\n",
    "        first_line = f.readline().rstrip()\n",
    "        robyn_first_lines.append(first_line)\n",
    "        \n",
    "#cher_first_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "1d31178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the list of songs from the first lines of the .txt files\n",
    "robyn_song_list = [song.replace('\"', '') for song in robyn_first_lines]\n",
    "robyn_song_list = [song.replace(\"'\", '') for song in robyn_song_list]\n",
    "#robyn_song_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "c684f922",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenize the individual words and clean for punctuation \n",
    "robyn_title_tokens = nltk.tokenize.word_tokenize(str(robyn_song_list))\n",
    "robyn_title_tokens = [token for token in robyn_title_tokens if token not in punctuation]\n",
    "#cher_title_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "5d243be9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Me', 11),\n",
       " ('You', 8),\n",
       " ('The', 7),\n",
       " ('My', 6),\n",
       " ('To', 6),\n",
       " (\"'Dont\", 4),\n",
       " ('U', 4),\n",
       " (\"'Love\", 3),\n",
       " (\"'Hang\", 3),\n",
       " ('With', 3)]"
      ]
     },
     "execution_count": 457,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count each hastag type and return the most common    \n",
    "count_song_title_words = []\n",
    "counter_title = Counter(robyn_title_tokens)\n",
    "top_10_words = counter_title.most_common(10)\n",
    "top_10_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "5f473913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe\n",
    "cher_songs_df = pd.DataFrame(columns=['filename', 'word_count'])\n",
    "\n",
    "# iterate through the list of files\n",
    "for song in cher_links:\n",
    "    with open(song, 'r') as f:\n",
    "        text = f.read()\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        #add a new row to the cher_songs_df\n",
    "        cher_songs_df = cher_songs_df.append({'filename': 'cher', 'word_count': word_count}, ignore_index=True)\n",
    "\n",
    "#print(cher_songs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "3d675e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe\n",
    "robyn_songs_df = pd.DataFrame(columns=['filename', 'word_count'])\n",
    "\n",
    "# iterate through the list of files\n",
    "for song in robyn_links:\n",
    "    with open(song, 'r') as f:\n",
    "        text = f.read()\n",
    "        words = text.split()\n",
    "        word_count = len(words)\n",
    "        \n",
    "        #add a new row to the cher_songs_df\n",
    "        robyn_songs_df = robyn_songs_df.append({'filename': 'robyn', 'word_count': word_count}, ignore_index=True)\n",
    "\n",
    "#print(robyn_songs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "id": "081ba77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "artist\n",
       "cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: word_count, dtype: object"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#concat the df's together\n",
    "full_songs_df = pd.concat([cher_songs_df,robyn_songs_df], ignore_index=True)\n",
    "\n",
    "#display the historgram\n",
    "full_songs_df.groupby('artist')['word_count'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "f0e34516",
   "metadata": {},
   "outputs": [],
   "source": [
    "collapse_whitespace = re.compile(r'\\s+')\n",
    "\n",
    "def tokenize_lyrics(lyric) : \n",
    "    \"\"\"strip and split on whitespace\"\"\"\n",
    "    return([item.lower() for item in collapse_whitespace.split(lyric)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294c440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your lyric length comparison chart here. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
